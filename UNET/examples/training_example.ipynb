{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60c83309-a2ab-4f0e-b078-b3a99643fe8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 13:25:10.848794: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-23 13:25:10.925617: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-23 13:25:12.271330: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "from keras.layers import Conv2D, Conv2DTranspose\n",
    "from keras.layers import Input\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import concatenate\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef2516e-a126-40ee-97f7-b9166244ffd5",
   "metadata": {},
   "source": [
    "# Training the U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0b741c-406b-439a-a704-dbf3c6b72e17",
   "metadata": {},
   "source": [
    "## Importing auxiliary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea2a00e-8730-4c2e-9c55-a8a40608b1b5",
   "metadata": {},
   "source": [
    "Here, we are adding the `root` directory in order to load a few auxiliary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c978446-02e3-4b54-b625-92b5abf838c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")  # fix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93d26a7-edc5-42cc-b6a8-715791731c2e",
   "metadata": {},
   "source": [
    "Now, we are importing some auxiliary functions:\n",
    " - `conv2d_block` is used to create a convolution block from parameters\n",
    " - `read_dataset` reads the labelled dataset (see `dataset_creation_example` notebook for reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f41d937-5a4f-45f6-942d-a39fb6e75698",
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.simple.conv2d_block import conv2d_block\n",
    "from training.simple.read_dataset import read_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74296146-ce4d-4d3c-8bca-ca813dcc2462",
   "metadata": {},
   "source": [
    "## Defining the path of the dataset\n",
    "\n",
    "After importing the auxiliary functions, it is necessary to define the path for the `dataset_folder`, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "236b0a78-51ea-4116-80e8-d7ba71e34244",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = Path(r\"../dataset/dataset_UNET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aa44b5-1381-4713-8300-35ac65c0bf4a",
   "metadata": {},
   "source": [
    "## Reading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28b27c2-412a-495b-bb5c-51046897a80b",
   "metadata": {},
   "source": [
    "Now, it's time to load the dataset. Here we are dividing the labelled samples (images and masks) into a \"Training\" and \"Validation\" dataset.\n",
    "Here, it's also necessary to define the `window_size`. This parameter is used to reshape the sub-images size (see `dataset_creation_example` notebook for reference). It is important to highlight that this parameter is used only to rescale images. **It does not change the sub-image area, it simply scales up or down the sub-images. If you do not want to rescale the image, simply let `window_size = sub_image_size`.**\n",
    "\n",
    "Taking the `dataset_creation_example` notebook as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77e10589-df92-4cee-becb-ba7a94ea55d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f4abb0e-c95b-4aab-a8e8-8d50c1e9c8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_train, masks_train = read_dataset(\n",
    "    dataset_folder, window_size=window_size, subset=\"Training\"\n",
    ")\n",
    "\n",
    "images_val, masks_val = read_dataset(\n",
    "    dataset_folder, window_size=window_size, subset=\"Validation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f16ef7-4d9f-45be-a9db-b3404ecb2083",
   "metadata": {},
   "source": [
    "After reading the dataset, we shuffle the dataset. Note that we are using a `seed` value of 13 to generate the \"random\" shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e14628dc-eb49-4675-9313-4a6b0f216648",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_dataset = images_train.shape[0]\n",
    "shuffle = np.arange(N_dataset)\n",
    "np.random.seed(13)\n",
    "np.random.shuffle(shuffle)\n",
    "images_train = images_train[shuffle, :, :, :]\n",
    "masks_train = masks_train[shuffle, :, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc1430b-5d6c-4dc6-8d76-425db2dcff07",
   "metadata": {},
   "source": [
    "## Defining the U-Net architecture\n",
    "\n",
    "Now that the training and validation data is available, we can now focus on defining the U-Net architecture.\n",
    "An example is shown below. For reference, this U-Net network is similar to the one shown in Fig. 4 of [this work](https://www.sciencedirect.com/science/article/pii/S0301932222002634#sec3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00e8490d-fa05-4c33-a80e-58695927cb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 13:25:14.253934: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "input_img = Input((window_size, window_size, 1), name=\"img\")\n",
    "\n",
    "c1 = conv2d_block(input_img, 8, kernel_size=5, batchnorm=True)\n",
    "p1 = MaxPooling2D((4, 4))(c1)\n",
    "\n",
    "c2 = conv2d_block(p1, 8, kernel_size=5, batchnorm=True)\n",
    "p2 = MaxPooling2D((4, 4))(c2)\n",
    "\n",
    "c3 = conv2d_block(p2, 8, kernel_size=5, batchnorm=True)\n",
    "\n",
    "u4 = Conv2DTranspose(8, kernel_size=5, strides=(4, 4), padding=\"same\")(c3)\n",
    "u4 = concatenate([u4, c2])\n",
    "c4 = conv2d_block(u4, 8, kernel_size=5, batchnorm=True)\n",
    "\n",
    "u5 = Conv2DTranspose(8, kernel_size=5, strides=(4, 4), padding=\"same\")(c4)\n",
    "u5 = concatenate([u5, c1])\n",
    "c5 = conv2d_block(u5, 8, kernel_size=5, batchnorm=True)\n",
    "\n",
    "outputs = Conv2D(1, (1, 1), activation=\"sigmoid\")(c5)\n",
    "model = Model(inputs=[input_img], outputs=[outputs])\n",
    "model.compile(optimizer=Adam(), loss=\"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d73c35d-98de-4e62-bf65-dec02e5a4821",
   "metadata": {},
   "source": [
    "Below we can get a summary of the model. Alternatively (and recommended), you can check the U-Net architecture with Keras' `plot_model` function (see [here](https://keras.io/api/utils/model_plotting_utils/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f896b3ce-c974-42fc-b87e-6d2ab4826855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ img (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">208</span> │ img[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │ conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,608</span> │ max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>) │         <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │ conv2d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_3        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,608</span> │ max_pooling2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │ conv2d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_5        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_transpose    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,608</span> │ activation_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_transpose… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │ activation_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,208</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>) │         <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │ conv2d_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_7        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_transpose_1  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,608</span> │ activation_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_transpose… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │ activation_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,208</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │ conv2d_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_9        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span> │ activation_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ img (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│                     │ \u001b[38;5;34m1\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │        \u001b[38;5;34m208\u001b[0m │ img[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "│                     │ \u001b[38;5;34m8\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │         \u001b[38;5;34m32\u001b[0m │ conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m8\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_1        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m8\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m8\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ activation_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m8\u001b[0m) │      \u001b[38;5;34m1,608\u001b[0m │ max_pooling2d[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m8\u001b[0m) │         \u001b[38;5;34m32\u001b[0m │ conv2d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_3        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m8\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ activation_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m)   │      \u001b[38;5;34m1,608\u001b[0m │ max_pooling2d_1[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m)   │         \u001b[38;5;34m32\u001b[0m │ conv2d_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_5        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_transpose    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m8\u001b[0m) │      \u001b[38;5;34m1,608\u001b[0m │ activation_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_transpose… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │ \u001b[38;5;34m16\u001b[0m)               │            │ activation_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m8\u001b[0m) │      \u001b[38;5;34m3,208\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m8\u001b[0m) │         \u001b[38;5;34m32\u001b[0m │ conv2d_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_7        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m8\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_transpose_1  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │      \u001b[38;5;34m1,608\u001b[0m │ activation_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)   │ \u001b[38;5;34m8\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ conv2d_transpose… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │ \u001b[38;5;34m16\u001b[0m)               │            │ activation_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │      \u001b[38;5;34m3,208\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │ \u001b[38;5;34m8\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │         \u001b[38;5;34m32\u001b[0m │ conv2d_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m8\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_9        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m8\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_10 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m9\u001b[0m │ activation_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ \u001b[38;5;34m1\u001b[0m)                │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,225</span> (51.66 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m13,225\u001b[0m (51.66 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,145</span> (51.35 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m13,145\u001b[0m (51.35 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> (320.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m80\u001b[0m (320.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4f4791-2343-4759-9f52-9593d7d2de84",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Now, with the U-Net model defined, it's time to train it!\n",
    "\n",
    "First, we setup a few callbacks to be executed during training.\n",
    "\n",
    "The first one is the `early_stopping` callback, which defines a `PATIENCE` value to limit the number of running epochs (`EPOCHS`).\n",
    "\n",
    "The second one, `best_model`, is a callback to store the U-Net with the lowest validation loss.\n",
    "\n",
    "The final, and third callback, is `TrackProgress()`, which tracks the training progress of the U-Net. \n",
    "It consists of image samples from the `Validation` dataset, showing: i) raw image; ii) manually labelled mask, and iii) U-Net generated mask. \n",
    "It's a useful tool to check the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "483d7259-dbed-4d02-812d-c2a34c4e422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATIENCE = 200\n",
    "EPOCHS = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "399d634a-ee75-4466-ad16-6a3a91e60399",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrackProgress(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        out_progress_folder = Path(\"progress/best\")\n",
    "        if epoch == 0:\n",
    "            out_progress_folder.mkdir(parents=True, exist_ok=True)\n",
    "        if epoch % 10 == 0:\n",
    "            _images = images_val[::16, :, :].copy()\n",
    "            _masks = masks_val[::16, :, :].copy()\n",
    "\n",
    "            n_tests = _images.shape[0]\n",
    "            _pred_masks = model.predict(_images)\n",
    "\n",
    "            _images *= 255.0\n",
    "            _masks *= 255.0\n",
    "            _pred_masks *= 255.0\n",
    "\n",
    "            for i in range(n_tests):\n",
    "                _img = _images[i, :, :, 0].astype(np.uint8)\n",
    "                _mask = _masks[i, :, :, 0].astype(np.uint8)\n",
    "                _pred_mask = _pred_masks[i, :, :, 0].astype(np.uint8)\n",
    "                _out_img = np.hstack((_img, _mask, _pred_mask))\n",
    "                cv2.imwrite(\n",
    "                    str(\n",
    "                        Path(\n",
    "                            out_progress_folder,\n",
    "                            f\"progress_img_ex_{i:03d}_{epoch:05d}.jpg\",\n",
    "                        )\n",
    "                    ),\n",
    "                    _out_img,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d96d432-3d26-4bdf-a14c-43ba48a1a15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\", mode=\"min\", verbose=1, patience=PATIENCE\n",
    ")\n",
    "\n",
    "best_model = ModelCheckpoint(\n",
    "    \"UNET_best.keras\", verbose=1, save_best_only=True, monitor=\"val_loss\", mode=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7203368-991c-45c5-9102-168f9184356c",
   "metadata": {},
   "source": [
    "Now that everything is defined, it's time to train the U-Net with our data. \n",
    "This is done below. It'll probably take a while to run...\n",
    "\n",
    "While training, you'll notice that a `progress/best` folder is created to monitor the training progress.\n",
    "Furthermore, the network with the lowest validation loss at a number of given epochs will be stored in a `UNET_best.keras` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d72d3e0d-4ccf-4c01-8cec-0e39b6e2a382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.52062, saving model to UNET_best.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265ms/step\n",
      "\n",
      "Epoch 2: val_loss improved from 0.52062 to 0.46848, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 0.46848 to 0.42592, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 4: val_loss improved from 0.42592 to 0.38403, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 5: val_loss improved from 0.38403 to 0.35628, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 6: val_loss improved from 0.35628 to 0.35041, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.35041\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.35041\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.35041\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.35041\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.35041\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.35041\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.35041\n",
      "\n",
      "Epoch 14: val_loss improved from 0.35041 to 0.33811, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.33811\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.33811\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.33811\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.33811\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.33811\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.33811\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.33811\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.33811\n",
      "\n",
      "Epoch 23: val_loss improved from 0.33811 to 0.33455, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 24: val_loss improved from 0.33455 to 0.32586, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.32586\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.32586\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.32586\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.32586\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.32586\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.32586\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.32586\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.32586\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.32586\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.32586\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.32586\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.32586\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.32586\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.32586\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.32586\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.32586\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.32586\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.32586\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.32586\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.32586\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.32586\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.32586\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.32586\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.32586\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.32586\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.32586\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.32586\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.32586\n",
      "\n",
      "Epoch 53: val_loss improved from 0.32586 to 0.32336, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 54: val_loss improved from 0.32336 to 0.30717, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 55: val_loss improved from 0.30717 to 0.30328, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 56: val_loss improved from 0.30328 to 0.29554, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 57: val_loss improved from 0.29554 to 0.28600, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 58: val_loss improved from 0.28600 to 0.27169, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 59: val_loss improved from 0.27169 to 0.26848, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 60: val_loss improved from 0.26848 to 0.25435, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 61: val_loss improved from 0.25435 to 0.23681, saving model to UNET_best.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.23681\n",
      "\n",
      "Epoch 63: val_loss improved from 0.23681 to 0.23656, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 64: val_loss improved from 0.23656 to 0.23042, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 65: val_loss improved from 0.23042 to 0.22496, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 66: val_loss improved from 0.22496 to 0.22361, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 67: val_loss improved from 0.22361 to 0.21858, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 68: val_loss improved from 0.21858 to 0.21583, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 69: val_loss improved from 0.21583 to 0.20882, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 70: val_loss improved from 0.20882 to 0.20833, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 71: val_loss improved from 0.20833 to 0.20120, saving model to UNET_best.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\n",
      "Epoch 72: val_loss improved from 0.20120 to 0.19807, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 73: val_loss improved from 0.19807 to 0.19324, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 74: val_loss improved from 0.19324 to 0.19021, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 75: val_loss improved from 0.19021 to 0.18952, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 76: val_loss improved from 0.18952 to 0.18404, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 77: val_loss improved from 0.18404 to 0.18185, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 78: val_loss improved from 0.18185 to 0.17901, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 79: val_loss improved from 0.17901 to 0.17585, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 80: val_loss improved from 0.17585 to 0.17390, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 81: val_loss improved from 0.17390 to 0.17230, saving model to UNET_best.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\n",
      "Epoch 82: val_loss improved from 0.17230 to 0.16977, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 83: val_loss improved from 0.16977 to 0.16641, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 84: val_loss improved from 0.16641 to 0.16408, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 85: val_loss improved from 0.16408 to 0.16225, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.16225\n",
      "\n",
      "Epoch 87: val_loss improved from 0.16225 to 0.16163, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 88: val_loss improved from 0.16163 to 0.15706, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 89: val_loss improved from 0.15706 to 0.15609, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 90: val_loss improved from 0.15609 to 0.15518, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 91: val_loss improved from 0.15518 to 0.15357, saving model to UNET_best.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\n",
      "Epoch 92: val_loss improved from 0.15357 to 0.15054, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 93: val_loss improved from 0.15054 to 0.14914, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 94: val_loss improved from 0.14914 to 0.14688, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.14688\n",
      "\n",
      "Epoch 96: val_loss improved from 0.14688 to 0.14494, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 97: val_loss improved from 0.14494 to 0.14310, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 98: val_loss improved from 0.14310 to 0.14131, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.14131\n",
      "\n",
      "Epoch 100: val_loss improved from 0.14131 to 0.13923, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 101: val_loss improved from 0.13923 to 0.13552, saving model to UNET_best.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.13552\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.13552\n",
      "\n",
      "Epoch 104: val_loss improved from 0.13552 to 0.13282, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 105: val_loss improved from 0.13282 to 0.13146, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 106: val_loss improved from 0.13146 to 0.13023, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 107: val_loss improved from 0.13023 to 0.12735, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 108: val_loss improved from 0.12735 to 0.12638, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.12638\n",
      "\n",
      "Epoch 110: val_loss improved from 0.12638 to 0.12587, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 111: val_loss improved from 0.12587 to 0.12026, saving model to UNET_best.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\n",
      "Epoch 112: val_loss improved from 0.12026 to 0.12014, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.12014\n",
      "\n",
      "Epoch 114: val_loss improved from 0.12014 to 0.11776, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 115: val_loss improved from 0.11776 to 0.11644, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 116: val_loss improved from 0.11644 to 0.11639, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 117: val_loss improved from 0.11639 to 0.11449, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 118: val_loss improved from 0.11449 to 0.11263, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 119: val_loss improved from 0.11263 to 0.11043, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 120: val_loss improved from 0.11043 to 0.10960, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 121: val_loss improved from 0.10960 to 0.10621, saving model to UNET_best.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.10621\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.10621\n",
      "\n",
      "Epoch 124: val_loss improved from 0.10621 to 0.10617, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 125: val_loss improved from 0.10617 to 0.10266, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.10266\n",
      "\n",
      "Epoch 127: val_loss improved from 0.10266 to 0.10228, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 128: val_loss improved from 0.10228 to 0.10061, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.10061\n",
      "\n",
      "Epoch 130: val_loss improved from 0.10061 to 0.09785, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 131: val_loss improved from 0.09785 to 0.09690, saving model to UNET_best.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.09690\n",
      "\n",
      "Epoch 133: val_loss improved from 0.09690 to 0.09620, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 134: val_loss improved from 0.09620 to 0.09320, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.09320\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.09320\n",
      "\n",
      "Epoch 137: val_loss improved from 0.09320 to 0.09148, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 138: val_loss improved from 0.09148 to 0.09007, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.09007\n",
      "\n",
      "Epoch 140: val_loss improved from 0.09007 to 0.08969, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 141: val_loss improved from 0.08969 to 0.08727, saving model to UNET_best.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.08727\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.08727\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.08727\n",
      "\n",
      "Epoch 145: val_loss improved from 0.08727 to 0.08464, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 146: val_loss improved from 0.08464 to 0.08293, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.08293\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.08293\n",
      "\n",
      "Epoch 149: val_loss improved from 0.08293 to 0.08179, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 150: val_loss improved from 0.08179 to 0.08174, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 151: val_loss improved from 0.08174 to 0.07931, saving model to UNET_best.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.07931\n",
      "\n",
      "Epoch 153: val_loss improved from 0.07931 to 0.07915, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.07915\n",
      "\n",
      "Epoch 155: val_loss improved from 0.07915 to 0.07906, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 156: val_loss improved from 0.07906 to 0.07649, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.07649\n",
      "\n",
      "Epoch 158: val_loss improved from 0.07649 to 0.07620, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.07620\n",
      "\n",
      "Epoch 160: val_loss improved from 0.07620 to 0.07387, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 161: val_loss improved from 0.07387 to 0.07232, saving model to UNET_best.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.07232\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.07232\n",
      "\n",
      "Epoch 164: val_loss improved from 0.07232 to 0.07098, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.07098\n",
      "\n",
      "Epoch 166: val_loss improved from 0.07098 to 0.07043, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 167: val_loss improved from 0.07043 to 0.06961, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.06961\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.06961\n",
      "\n",
      "Epoch 170: val_loss improved from 0.06961 to 0.06944, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 171: val_loss improved from 0.06944 to 0.06911, saving model to UNET_best.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\n",
      "Epoch 172: val_loss improved from 0.06911 to 0.06898, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 173: val_loss improved from 0.06898 to 0.06621, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.06621\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.06621\n",
      "\n",
      "Epoch 176: val_loss improved from 0.06621 to 0.06391, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.06391\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.06391\n",
      "\n",
      "Epoch 179: val_loss improved from 0.06391 to 0.06388, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.06388\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.06388\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\n",
      "Epoch 182: val_loss improved from 0.06388 to 0.06355, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.06355\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.06355\n",
      "\n",
      "Epoch 185: val_loss improved from 0.06355 to 0.06258, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.06258\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.06258\n",
      "\n",
      "Epoch 188: val_loss improved from 0.06258 to 0.06090, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.06090\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.06090\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.06090\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.06090\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.06090\n",
      "\n",
      "Epoch 194: val_loss improved from 0.06090 to 0.05947, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 195: val_loss improved from 0.05947 to 0.05932, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.05932\n",
      "\n",
      "Epoch 197: val_loss improved from 0.05932 to 0.05906, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.05906\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.05906\n",
      "\n",
      "Epoch 200: val_loss improved from 0.05906 to 0.05725, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.05725\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\n",
      "Epoch 202: val_loss improved from 0.05725 to 0.05638, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.05638\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.05638\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.05638\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.05638\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.05638\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.05638\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.05638\n",
      "\n",
      "Epoch 210: val_loss improved from 0.05638 to 0.05459, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.05459\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.05459\n",
      "\n",
      "Epoch 213: val_loss improved from 0.05459 to 0.05385, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.05385\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.05385\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.05385\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.05385\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.05385\n",
      "\n",
      "Epoch 219: val_loss improved from 0.05385 to 0.05272, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.05272\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.05272\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\n",
      "Epoch 222: val_loss improved from 0.05272 to 0.05185, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.05185\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.05185\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.05185\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.05185\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.05185\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.05185\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.05185\n",
      "\n",
      "Epoch 230: val_loss improved from 0.05185 to 0.05034, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.05034\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.05034\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.05034\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.05034\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.05034\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.05034\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.05034\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.05034\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.05034\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.05034\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.05034\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\n",
      "Epoch 242: val_loss improved from 0.05034 to 0.05023, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.05023\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.05023\n",
      "\n",
      "Epoch 245: val_loss improved from 0.05023 to 0.04834, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.04834\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.04834\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.04834\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.04834\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.04834\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.04834\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.04834\n",
      "\n",
      "Epoch 253: val_loss improved from 0.04834 to 0.04755, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.04755\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.04755\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.04755\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.04755\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.04755\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.04755\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.04755\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.04755\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.04755\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.04755\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.04755\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.04755\n",
      "\n",
      "Epoch 266: val_loss improved from 0.04755 to 0.04726, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.04726\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.04726\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.04726\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.04726\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.04726\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.04726\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.04726\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.04726\n",
      "\n",
      "Epoch 275: val_loss improved from 0.04726 to 0.04619, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.04619\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.04619\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.04619\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.04619\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.04619\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.04619\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.04619\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.04619\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.04619\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.04619\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.04619\n",
      "\n",
      "Epoch 287: val_loss improved from 0.04619 to 0.04575, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.04575\n",
      "\n",
      "Epoch 289: val_loss improved from 0.04575 to 0.04445, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.04445\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.04445\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.04445\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.04445\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.04445\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.04445\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.04445\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.04445\n",
      "\n",
      "Epoch 298: val_loss improved from 0.04445 to 0.04445, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.04445\n",
      "\n",
      "Epoch 300: val_loss improved from 0.04445 to 0.04438, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 301: val_loss did not improve from 0.04438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\n",
      "Epoch 302: val_loss did not improve from 0.04438\n",
      "\n",
      "Epoch 303: val_loss improved from 0.04438 to 0.04427, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 304: val_loss improved from 0.04427 to 0.04400, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 305: val_loss did not improve from 0.04400\n",
      "\n",
      "Epoch 306: val_loss did not improve from 0.04400\n",
      "\n",
      "Epoch 307: val_loss did not improve from 0.04400\n",
      "\n",
      "Epoch 308: val_loss did not improve from 0.04400\n",
      "\n",
      "Epoch 309: val_loss did not improve from 0.04400\n",
      "\n",
      "Epoch 310: val_loss did not improve from 0.04400\n",
      "\n",
      "Epoch 311: val_loss improved from 0.04400 to 0.04332, saving model to UNET_best.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\n",
      "Epoch 312: val_loss did not improve from 0.04332\n",
      "\n",
      "Epoch 313: val_loss did not improve from 0.04332\n",
      "\n",
      "Epoch 314: val_loss did not improve from 0.04332\n",
      "\n",
      "Epoch 315: val_loss did not improve from 0.04332\n",
      "\n",
      "Epoch 316: val_loss improved from 0.04332 to 0.04246, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 317: val_loss did not improve from 0.04246\n",
      "\n",
      "Epoch 318: val_loss did not improve from 0.04246\n",
      "\n",
      "Epoch 319: val_loss did not improve from 0.04246\n",
      "\n",
      "Epoch 320: val_loss did not improve from 0.04246\n",
      "\n",
      "Epoch 321: val_loss did not improve from 0.04246\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\n",
      "Epoch 322: val_loss did not improve from 0.04246\n",
      "\n",
      "Epoch 323: val_loss did not improve from 0.04246\n",
      "\n",
      "Epoch 324: val_loss improved from 0.04246 to 0.04221, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 325: val_loss did not improve from 0.04221\n",
      "\n",
      "Epoch 326: val_loss did not improve from 0.04221\n",
      "\n",
      "Epoch 327: val_loss did not improve from 0.04221\n",
      "\n",
      "Epoch 328: val_loss did not improve from 0.04221\n",
      "\n",
      "Epoch 329: val_loss improved from 0.04221 to 0.04160, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 330: val_loss did not improve from 0.04160\n",
      "\n",
      "Epoch 331: val_loss did not improve from 0.04160\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\n",
      "Epoch 332: val_loss did not improve from 0.04160\n",
      "\n",
      "Epoch 333: val_loss did not improve from 0.04160\n",
      "\n",
      "Epoch 334: val_loss did not improve from 0.04160\n",
      "\n",
      "Epoch 335: val_loss did not improve from 0.04160\n",
      "\n",
      "Epoch 336: val_loss did not improve from 0.04160\n",
      "\n",
      "Epoch 337: val_loss did not improve from 0.04160\n",
      "\n",
      "Epoch 338: val_loss did not improve from 0.04160\n",
      "\n",
      "Epoch 339: val_loss did not improve from 0.04160\n",
      "\n",
      "Epoch 340: val_loss did not improve from 0.04160\n",
      "\n",
      "Epoch 341: val_loss did not improve from 0.04160\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\n",
      "Epoch 342: val_loss did not improve from 0.04160\n",
      "\n",
      "Epoch 343: val_loss did not improve from 0.04160\n",
      "\n",
      "Epoch 344: val_loss did not improve from 0.04160\n",
      "\n",
      "Epoch 345: val_loss did not improve from 0.04160\n",
      "\n",
      "Epoch 346: val_loss did not improve from 0.04160\n",
      "\n",
      "Epoch 347: val_loss improved from 0.04160 to 0.04140, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 348: val_loss did not improve from 0.04140\n",
      "\n",
      "Epoch 349: val_loss did not improve from 0.04140\n",
      "\n",
      "Epoch 350: val_loss did not improve from 0.04140\n",
      "\n",
      "Epoch 351: val_loss did not improve from 0.04140\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\n",
      "Epoch 352: val_loss did not improve from 0.04140\n",
      "\n",
      "Epoch 353: val_loss did not improve from 0.04140\n",
      "\n",
      "Epoch 354: val_loss did not improve from 0.04140\n",
      "\n",
      "Epoch 355: val_loss did not improve from 0.04140\n",
      "\n",
      "Epoch 356: val_loss did not improve from 0.04140\n",
      "\n",
      "Epoch 357: val_loss did not improve from 0.04140\n",
      "\n",
      "Epoch 358: val_loss did not improve from 0.04140\n",
      "\n",
      "Epoch 359: val_loss did not improve from 0.04140\n",
      "\n",
      "Epoch 360: val_loss did not improve from 0.04140\n",
      "\n",
      "Epoch 361: val_loss did not improve from 0.04140\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\n",
      "Epoch 362: val_loss did not improve from 0.04140\n",
      "\n",
      "Epoch 363: val_loss improved from 0.04140 to 0.04124, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 364: val_loss did not improve from 0.04124\n",
      "\n",
      "Epoch 365: val_loss did not improve from 0.04124\n",
      "\n",
      "Epoch 366: val_loss did not improve from 0.04124\n",
      "\n",
      "Epoch 367: val_loss did not improve from 0.04124\n",
      "\n",
      "Epoch 368: val_loss did not improve from 0.04124\n",
      "\n",
      "Epoch 369: val_loss did not improve from 0.04124\n",
      "\n",
      "Epoch 370: val_loss did not improve from 0.04124\n",
      "\n",
      "Epoch 371: val_loss did not improve from 0.04124\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\n",
      "Epoch 372: val_loss did not improve from 0.04124\n",
      "\n",
      "Epoch 373: val_loss did not improve from 0.04124\n",
      "\n",
      "Epoch 374: val_loss did not improve from 0.04124\n",
      "\n",
      "Epoch 375: val_loss did not improve from 0.04124\n",
      "\n",
      "Epoch 376: val_loss did not improve from 0.04124\n",
      "\n",
      "Epoch 377: val_loss did not improve from 0.04124\n",
      "\n",
      "Epoch 378: val_loss did not improve from 0.04124\n",
      "\n",
      "Epoch 379: val_loss did not improve from 0.04124\n",
      "\n",
      "Epoch 380: val_loss did not improve from 0.04124\n",
      "\n",
      "Epoch 381: val_loss did not improve from 0.04124\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\n",
      "Epoch 382: val_loss did not improve from 0.04124\n",
      "\n",
      "Epoch 383: val_loss did not improve from 0.04124\n",
      "\n",
      "Epoch 384: val_loss did not improve from 0.04124\n",
      "\n",
      "Epoch 385: val_loss did not improve from 0.04124\n",
      "\n",
      "Epoch 386: val_loss did not improve from 0.04124\n",
      "\n",
      "Epoch 387: val_loss did not improve from 0.04124\n",
      "\n",
      "Epoch 388: val_loss did not improve from 0.04124\n",
      "\n",
      "Epoch 389: val_loss did not improve from 0.04124\n",
      "\n",
      "Epoch 390: val_loss did not improve from 0.04124\n",
      "\n",
      "Epoch 391: val_loss did not improve from 0.04124\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\n",
      "Epoch 392: val_loss did not improve from 0.04124\n",
      "\n",
      "Epoch 393: val_loss did not improve from 0.04124\n",
      "\n",
      "Epoch 394: val_loss did not improve from 0.04124\n",
      "\n",
      "Epoch 395: val_loss did not improve from 0.04124\n",
      "\n",
      "Epoch 396: val_loss did not improve from 0.04124\n",
      "\n",
      "Epoch 397: val_loss did not improve from 0.04124\n",
      "\n",
      "Epoch 398: val_loss did not improve from 0.04124\n",
      "\n",
      "Epoch 399: val_loss did not improve from 0.04124\n",
      "\n",
      "Epoch 400: val_loss did not improve from 0.04124\n",
      "\n",
      "Epoch 401: val_loss did not improve from 0.04124\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\n",
      "Epoch 402: val_loss did not improve from 0.04124\n",
      "\n",
      "Epoch 403: val_loss did not improve from 0.04124\n",
      "\n",
      "Epoch 404: val_loss improved from 0.04124 to 0.04112, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 405: val_loss did not improve from 0.04112\n",
      "\n",
      "Epoch 406: val_loss did not improve from 0.04112\n",
      "\n",
      "Epoch 407: val_loss did not improve from 0.04112\n",
      "\n",
      "Epoch 408: val_loss did not improve from 0.04112\n",
      "\n",
      "Epoch 409: val_loss did not improve from 0.04112\n",
      "\n",
      "Epoch 410: val_loss did not improve from 0.04112\n",
      "\n",
      "Epoch 411: val_loss did not improve from 0.04112\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\n",
      "Epoch 412: val_loss did not improve from 0.04112\n",
      "\n",
      "Epoch 413: val_loss improved from 0.04112 to 0.04097, saving model to UNET_best.keras\n",
      "\n",
      "Epoch 414: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 415: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 416: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 417: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 418: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 419: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 420: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 421: val_loss did not improve from 0.04097\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\n",
      "Epoch 422: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 423: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 424: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 425: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 426: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 427: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 428: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 429: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 430: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 431: val_loss did not improve from 0.04097\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\n",
      "Epoch 432: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 433: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 434: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 435: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 436: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 437: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 438: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 439: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 440: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 441: val_loss did not improve from 0.04097\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\n",
      "Epoch 442: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 443: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 444: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 445: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 446: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 447: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 448: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 449: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 450: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 451: val_loss did not improve from 0.04097\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\n",
      "Epoch 452: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 453: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 454: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 455: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 456: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 457: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 458: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 459: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 460: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 461: val_loss did not improve from 0.04097\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\n",
      "Epoch 462: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 463: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 464: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 465: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 466: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 467: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 468: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 469: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 470: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 471: val_loss did not improve from 0.04097\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\n",
      "Epoch 472: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 473: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 474: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 475: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 476: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 477: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 478: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 479: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 480: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 481: val_loss did not improve from 0.04097\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\n",
      "Epoch 482: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 483: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 484: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 485: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 486: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 487: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 488: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 489: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 490: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 491: val_loss did not improve from 0.04097\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\n",
      "Epoch 492: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 493: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 494: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 495: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 496: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 497: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 498: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 499: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 500: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 501: val_loss did not improve from 0.04097\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\n",
      "Epoch 502: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 503: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 504: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 505: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 506: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 507: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 508: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 509: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 510: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 511: val_loss did not improve from 0.04097\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\n",
      "Epoch 512: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 513: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 514: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 515: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 516: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 517: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 518: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 519: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 520: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 521: val_loss did not improve from 0.04097\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\n",
      "Epoch 522: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 523: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 524: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 525: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 526: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 527: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 528: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 529: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 530: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 531: val_loss did not improve from 0.04097\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\n",
      "Epoch 532: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 533: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 534: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 535: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 536: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 537: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 538: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 539: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 540: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 541: val_loss did not improve from 0.04097\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\n",
      "Epoch 542: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 543: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 544: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 545: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 546: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 547: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 548: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 549: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 550: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 551: val_loss did not improve from 0.04097\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\n",
      "Epoch 552: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 553: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 554: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 555: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 556: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 557: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 558: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 559: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 560: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 561: val_loss did not improve from 0.04097\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\n",
      "Epoch 562: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 563: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 564: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 565: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 566: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 567: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 568: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 569: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 570: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 571: val_loss did not improve from 0.04097\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\n",
      "Epoch 572: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 573: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 574: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 575: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 576: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 577: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 578: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 579: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 580: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 581: val_loss did not improve from 0.04097\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\n",
      "Epoch 582: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 583: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 584: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 585: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 586: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 587: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 588: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 589: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 590: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 591: val_loss did not improve from 0.04097\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\n",
      "Epoch 592: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 593: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 594: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 595: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 596: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 597: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 598: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 599: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 600: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 601: val_loss did not improve from 0.04097\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\n",
      "Epoch 602: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 603: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 604: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 605: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 606: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 607: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 608: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 609: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 610: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 611: val_loss did not improve from 0.04097\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\n",
      "Epoch 612: val_loss did not improve from 0.04097\n",
      "\n",
      "Epoch 613: val_loss did not improve from 0.04097\n",
      "Epoch 613: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f5ad52bd330>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "model.fit(\n",
    "    images_train,\n",
    "    masks_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(images_val, masks_val),\n",
    "    callbacks=[early_stopping, best_model, TrackProgress()],\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "attachments": {
    "02a0ab65-a728-4d46-ba92-c27e10072380.jpg": {
     "image/jpeg": "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"
    },
    "044e9841-0065-4968-b3f2-c73f738389ad.jpg": {
     "image/jpeg": "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"
    },
    "71a65034-eba4-4a41-8b95-aa0a70f472c2.jpg": {
     "image/jpeg": "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"
    },
    "f89e0c97-5507-4d4f-9368-66ffec8bc549.jpg": {
     "image/jpeg": "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"
    }
   },
   "cell_type": "markdown",
   "id": "442e4a46-1131-4f90-acd5-cf592a5c1b85",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "At this point, the model is trained. In order to use the model as a segmentation tool, please check the `predicting_example`.\n",
    "\n",
    "As a reference, look at the U-Net generated segmentation masks obtained in selected epochs during training.\n",
    "\n",
    "### Epoch 0000:\n",
    "![progress_img_ex_000_00000.jpg](attachment:044e9841-0065-4968-b3f2-c73f738389ad.jpg)\n",
    "\n",
    "### Epoch 0060:\n",
    "![progress_img_ex_000_00060.jpg](attachment:02a0ab65-a728-4d46-ba92-c27e10072380.jpg)\n",
    "\n",
    "### Epoch 0100:\n",
    "![progress_img_ex_000_00100.jpg](attachment:f89e0c97-5507-4d4f-9368-66ffec8bc549.jpg)\n",
    "\n",
    "### Epoch 0580:\n",
    "![progress_img_ex_000_00580.jpg](attachment:71a65034-eba4-4a41-8b95-aa0a70f472c2.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da82dc54-74ad-4062-8fbf-796dd10a6ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
