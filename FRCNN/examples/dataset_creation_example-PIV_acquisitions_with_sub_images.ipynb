{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd5a892e-d2eb-4342-938f-b0a97777c433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import random\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130ccac0-c323-428b-8b0d-45bb12c6e5de",
   "metadata": {},
   "source": [
    "# F-RCNN Dataset Generation\n",
    "## PIV Acquisitions - example with sub-images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4953565a-4c71-405c-a2ea-d4837e95cddb",
   "metadata": {},
   "source": [
    "## Importing the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ff24ef2-ffbd-487e-a12b-3884f405c88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-26 18:58:37.037040: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-26 18:58:37.060856: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-26 18:58:37.089061: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-26 18:58:37.097199: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-26 18:58:37.116965: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-26 18:58:38.626741: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(\"..\")  # TODO: Fix it!\n",
    "from dataset.create_dataset_sub_images import create_dataset_sub_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996c618a-3fa5-4a0b-a8e1-5b1cb91d3933",
   "metadata": {},
   "source": [
    "## Defining the path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35afbf2-6430-4113-8d6e-9b1623fad321",
   "metadata": {},
   "source": [
    "A `.h5` file containing a F-RCNN example is provided in this example (`PIV_annotation_files/PIV_dataset.h5`).\n",
    "\n",
    "The following F-RCNN dataset was created using the image annotation tool available [here](https://github.com/sinmec/multilabellerg).\n",
    "\n",
    "The current implementation can read multiple `.h5` files. This is done by simply placing the dataset files in the same `h5_dataset_path` folder.\n",
    "\n",
    "The images are extracted to the `output_path` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6837dda-5643-40fb-81bb-9672e4f2e885",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_dataset_path = Path('PIV_annotation_files')\n",
    "output_path = Path(r\"example_dataset_FRCNN_PIV_subimage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78c9c62-ddf2-4b03-b1dc-76d3fd3d541e",
   "metadata": {},
   "source": [
    "## Defining parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1998954a-e44b-4baa-bde9-3dc330dfef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Validation images\n",
    "N_VALIDATION = 2\n",
    "\n",
    "# Number of Verification images\n",
    "N_VERIFICATION = 2\n",
    "\n",
    "# Sub-image size\n",
    "WINDOW_SIZE = 512\n",
    "\n",
    "# Number of random samples of (WINDOW_SIZE, WINDOW_SIZE) shape\n",
    "N_RANDOM_SUB_IMAGES = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe68285-d583-4d0f-9689-6243bd2396c0",
   "metadata": {},
   "source": [
    "For the dataset generation, two options are provided.\n",
    "\n",
    "In the first one, the dataset is composed of the annotated contours and the original images, i.e., the F-RCNN object detection is based on the original image information. This option may work well if the image is not complex and the objects are easily distinguished.\n",
    "For this option, simply define the variable  `UNET_model_options = None`, as shown (and commented) below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68cd1632-99f1-4ac7-bab5-c5313e95e49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNET_model_options = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75826b0c-dea6-422d-91f9-b6732fea2bf8",
   "metadata": {},
   "source": [
    "In some cases, for instance, when dealing with [bubbly flow PIV acqusitions](https://www.sciencedirect.com/science/article/pii/S0009250918303269), the objects are not readily visible in the original image.\n",
    "In this case, the F-RCNN object detection is based on an intermediate representation of the original image. Here, we apply a U-Net model to segment the image and highlight the bubble positions. For this option, the `UNET_model_options` should be configured.\n",
    "It is important to note that, to use this option, it is required to train an appropriate U-Net model, following the guidelines detailed in this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb70c1ee-70ab-423c-97e8-4882728c6a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNET_model_options = {'keras_file': Path('PIV_annotation_files', 'UNET_model.keras'),\n",
    "                      'window_size': 256,\n",
    "                      'sub_image_size': 256,\n",
    "                      'stride_division': 16}                      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bef62b-0b6b-4352-9641-79d1c8060e4a",
   "metadata": {},
   "source": [
    "## Generating the dataset\n",
    "\n",
    "The dataset is generated from the `create_dataset` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdbf341a-6e02-410f-9973-855bc97e4618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-26 18:58:41.363687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3820 MB memory:  -> device: 0, name: NVIDIA RTX A2000, pci bus id: 0000:b3:00.0, compute capability: 8.6\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1727387924.824853 2314538 service.cc:146] XLA service 0x7f86f0004810 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1727387924.824947 2314538 service.cc:154]   StreamExecutor device (0): NVIDIA RTX A2000, Compute Capability 8.6\n",
      "2024-09-26 18:58:44.854478: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-09-26 18:58:44.991627: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n",
      "I0000 00:00:1727387927.387832 2314538 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "/home/rafaelfc/Data/DeepFlowImaging/FRCNN/examples/../../UNET/prediction/recreate_UNET_image.py:40: RuntimeWarning: invalid value encountered in divide\n",
      "  UNET_image /= UNET_image_nmax\n",
      "Cropping image 1/11: 100%|######################| 20/20 [00:02<00:00,  7.43it/s]\n",
      "Cropping image 2/11: 100%|######################| 20/20 [00:01<00:00, 10.26it/s]\n",
      "Cropping image 3/11: 100%|######################| 20/20 [00:02<00:00,  8.44it/s]\n",
      "Cropping image 4/11: 100%|######################| 20/20 [00:02<00:00,  6.71it/s]\n",
      "Cropping image 5/11: 100%|######################| 20/20 [00:04<00:00,  4.64it/s]\n",
      "Cropping image 6/11: 100%|######################| 20/20 [00:03<00:00,  6.57it/s]\n",
      "Cropping image 7/11: 100%|######################| 20/20 [00:04<00:00,  4.92it/s]\n",
      "Cropping image 8/11: 100%|######################| 20/20 [00:03<00:00,  5.44it/s]\n",
      "Cropping image 9/11: 100%|######################| 20/20 [00:03<00:00,  5.53it/s]\n",
      "Cropping image 10/11: 100%|#####################| 20/20 [00:03<00:00,  5.38it/s]\n",
      "Cropping image 11/11: 100%|#####################| 20/20 [00:03<00:00,  5.65it/s]\n"
     ]
    }
   ],
   "source": [
    "create_dataset_sub_images(h5_dataset_path, output_path, \n",
    "                          N_VALIDATION, N_VERIFICATION, \n",
    "                          N_RANDOM_SUB_IMAGES, WINDOW_SIZE, \n",
    "                          UNET_model_options=UNET_model_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101d3438-b91a-4b2d-9efc-f526596a65f6",
   "metadata": {},
   "source": [
    "By running this function, you'll see that it created three new folders on the chosen path:\n",
    " - Training: Samples used during the F-RCNN training\n",
    " - Validation: Samples used for validation purpouses during training - Total of `N_VALIDATION` full images\n",
    " - Verification: Samples used to evaluate the F-RCNN accuracy after the training step. Unseen data during training  - Total of `N_VERIFICATION` full images\n",
    "\n",
    "\n",
    "\n",
    "## Option 1\n",
    "**If the `UNET_model_options` <span style=\"color:red\">is not</span> defined**, the F-RCNN object detection is based on the original image. \n",
    "In this case, folder cotains 3 folders:\n",
    " - `contours`: Text files which contains coordinates and parameters of the contours\n",
    " - `debug`: All the labelled images from the `.h5` file\n",
    " - `images`: All the raw images from the `.h5` file\n",
    "\n",
    "## Option 2\n",
    "**If the `UNET_model_options` <span style=\"color:red\">is </span> defined**, the F-RCNN object detection is based on the U-Net (segmented) representation of the image. \n",
    "In this case, folder cotains 4 folders:\n",
    " - `contours`: Text files which contains coordinates and parameters of the contours\n",
    " - `debug`: All the labelled images from the `.h5` file\n",
    " - `images`: All the raw images from the `.h5` file\n",
    " - `masks`: U-Net segmented images from images originally labeled in the `.h5` file\n",
    "\n",
    "In both options, the folder created in this step should be sent to the `FRCNN/dataset` folder for training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
