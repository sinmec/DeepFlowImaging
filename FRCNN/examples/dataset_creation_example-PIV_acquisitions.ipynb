{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5a892e-d2eb-4342-938f-b0a97777c433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import random\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130ccac0-c323-428b-8b0d-45bb12c6e5de",
   "metadata": {},
   "source": [
    "# F-RCNN Dataset Generation\n",
    "## PIV Acquisitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4953565a-4c71-405c-a2ea-d4837e95cddb",
   "metadata": {},
   "source": [
    "## Importing the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff24ef2-ffbd-487e-a12b-3884f405c88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")  # TODO: Fix it!\n",
    "from dataset.create_dataset import create_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996c618a-3fa5-4a0b-a8e1-5b1cb91d3933",
   "metadata": {},
   "source": [
    "## Defining the path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35afbf2-6430-4113-8d6e-9b1623fad321",
   "metadata": {},
   "source": [
    "A `.h5` file containing a F-RCNN example is provided in this example (`PIV_annotation_files/PIV_dataset.h5`).\n",
    "\n",
    "The following F-RCNN dataset was created using the image annotation tool available [here](https://github.com/sinmec/multilabellerg).\n",
    "\n",
    "The current implementation can read multiple `.h5` files. This is done by simply placing the dataset files in the same `h5_dataset_path` folder.\n",
    "\n",
    "The images are extracted to the `output_path` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6837dda-5643-40fb-81bb-9672e4f2e885",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_dataset_path = Path('PIV_annotation_files')\n",
    "output_path = Path(r\"example_dataset_FRCNN_PIV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78c9c62-ddf2-4b03-b1dc-76d3fd3d541e",
   "metadata": {},
   "source": [
    "## Defining parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1998954a-e44b-4baa-bde9-3dc330dfef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Validation images\n",
    "N_VALIDATION = 2\n",
    "\n",
    "# Number of Verification images\n",
    "N_VERIFICATION = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe68285-d583-4d0f-9689-6243bd2396c0",
   "metadata": {},
   "source": [
    "For the dataset generation, two options are provided.\n",
    "\n",
    "In the first one, the dataset is composed of the annotated contours and the original images, i.e., the F-RCNN object detection is based on the original image information. This option may work well if the image is not complex and the objects are easily distinguished.\n",
    "For this option, simply define the variable  `UNET_model_options = None`, as shown (and commented) below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cd1632-99f1-4ac7-bab5-c5313e95e49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNET_model_options = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75826b0c-dea6-422d-91f9-b6732fea2bf8",
   "metadata": {},
   "source": [
    "In some cases, for instance, when dealing with [bubbly flow PIV acqusitions](https://www.sciencedirect.com/science/article/pii/S0009250918303269), the objects are not readily visible in the original image.\n",
    "In this case, the F-RCNN object detection is based on an intermediate representation of the original image. Here, we apply a U-Net model to segment the image and highlight the bubble positions. For this option, the `UNET_model_options` should be configured.\n",
    "It is important to note that, to use this option, it is required to train an appropriate U-Net model, following the guidelines detailed in this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb70c1ee-70ab-423c-97e8-4882728c6a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNET_model_options = {'keras_file': Path('PIV_annotation_files', 'UNET_model.keras'),\n",
    "                      'window_size': 512,\n",
    "                      'sub_image_size': 128,\n",
    "                      'stride_division': 16}                      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bef62b-0b6b-4352-9641-79d1c8060e4a",
   "metadata": {},
   "source": [
    "## Generating the dataset\n",
    "\n",
    "The dataset is generated from the `create_dataset` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbf341a-6e02-410f-9973-855bc97e4618",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset(h5_dataset_path, output_path, N_VALIDATION, N_VERIFICATION, UNET_model_options=UNET_model_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101d3438-b91a-4b2d-9efc-f526596a65f6",
   "metadata": {},
   "source": [
    "By running this function, you'll see that it created three new folders on the chosen path:\n",
    " - Training: Samples used during the F-RCNN training\n",
    " - Validation: Samples used for validation purpouses during training - Total of `N_VALIDATION` full images\n",
    " - Verification: Samples used to evaluate the F-RCNN accuracy after the training step. Unseen data during training  - Total of `N_VERIFICATION` full images\n",
    "\n",
    "\n",
    "\n",
    "## Option 1\n",
    "**If the `UNET_model_options` <span style=\"color:red\">is not</span> defined**, the F-RCNN object detection is based on the original image. \n",
    "In this case, folder cotains 3 folders:\n",
    " - `contours`: Text files which contains coordinates and parameters of the contours\n",
    " - `debug`: All the labelled images from the `.h5` file\n",
    " - `images`: All the raw images from the `.h5` file\n",
    "\n",
    "## Option 2\n",
    "**If the `UNET_model_options` <span style=\"color:red\">is </span> defined**, the F-RCNN object detection is based on the U-Net (segmented) representation of the image. \n",
    "In this case, folder cotains 4 folders:\n",
    " - `contours`: Text files which contains coordinates and parameters of the contours\n",
    " - `debug`: All the labelled images from the `.h5` file\n",
    " - `images`: All the raw images from the `.h5` file\n",
    " - `masks`: U-Net segmented images from images originally labeled in the `.h5` file\n",
    "\n",
    "In both options, the folder created in this step should be sent to the `FRCNN/dataset` folder for training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
