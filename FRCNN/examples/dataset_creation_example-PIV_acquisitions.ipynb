{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd5a892e-d2eb-4342-938f-b0a97777c433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import random\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130ccac0-c323-428b-8b0d-45bb12c6e5de",
   "metadata": {},
   "source": [
    "# F-RCNN Dataset Generation\n",
    "## PIV Acquisitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4953565a-4c71-405c-a2ea-d4837e95cddb",
   "metadata": {},
   "source": [
    "## Importing the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ff24ef2-ffbd-487e-a12b-3884f405c88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-26 13:52:51.056125: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-26 13:52:51.079814: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-26 13:52:51.114141: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-26 13:52:51.114210: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-26 13:52:51.135904: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-26 13:52:52.527750: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(\"..\")  # TODO: Fix it!\n",
    "from dataset.create_dataset import create_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996c618a-3fa5-4a0b-a8e1-5b1cb91d3933",
   "metadata": {},
   "source": [
    "## Defining the path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35afbf2-6430-4113-8d6e-9b1623fad321",
   "metadata": {},
   "source": [
    "A `.h5` file containing a F-RCNN example is provided in this example (`PIV_annotation_files/PIV_dataset.h5`).\n",
    "\n",
    "The following F-RCNN dataset was created using the image annotation tool available [here](https://github.com/sinmec/multilabellerg).\n",
    "\n",
    "The current implementation can read multiple `.h5` files. This is done by simply placing the dataset files in the same `h5_dataset_path` folder.\n",
    "\n",
    "The images are extracted to the `output_path` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6837dda-5643-40fb-81bb-9672e4f2e885",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_dataset_path = Path('PIV_annotation_files')\n",
    "output_path = Path(r\"example_dataset_FRCNN_PIV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78c9c62-ddf2-4b03-b1dc-76d3fd3d541e",
   "metadata": {},
   "source": [
    "## Defining parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1998954a-e44b-4baa-bde9-3dc330dfef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Validation images\n",
    "N_VALIDATION = 2\n",
    "\n",
    "# Number of Verification images\n",
    "N_VERIFICATION = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe68285-d583-4d0f-9689-6243bd2396c0",
   "metadata": {},
   "source": [
    "For the dataset generation, two options are provided.\n",
    "\n",
    "In the first one, the dataset is composed of the annotated contours and the original images, i.e., the F-RCNN object detection is based on the original image information. This option may work well if the image is not complex and the objects are easily distinguished.\n",
    "For this option, simply define the variable  `UNET_model_options = None`, as shown (and commented) below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68cd1632-99f1-4ac7-bab5-c5313e95e49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNET_model_options = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75826b0c-dea6-422d-91f9-b6732fea2bf8",
   "metadata": {},
   "source": [
    "In some cases, for instance, when dealing with [bubbly flow PIV acqusitions](https://www.sciencedirect.com/science/article/pii/S0009250918303269), the objects are not readily visible in the original image.\n",
    "In this case, the F-RCNN object detection is based on an intermediate representation of the original image. Here, we apply a U-Net model to segment the image and highlight the bubble positions. For this option, the `UNET_model_options` should be configured.\n",
    "It is important to note that, to use this option, it is required to train an appropriate U-Net model, following the guidelines detailed in this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb70c1ee-70ab-423c-97e8-4882728c6a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNET_model_options = {'keras_file': Path('PIV_annotation_files', 'UNET_model.keras'),\n",
    "                      'window_size': 256,\n",
    "                      'sub_image_size': 256,\n",
    "                      'stride_division': 16}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bef62b-0b6b-4352-9641-79d1c8060e4a",
   "metadata": {},
   "source": [
    "## Generating the dataset\n",
    "\n",
    "The dataset is generated from the `create_dataset` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdbf341a-6e02-410f-9973-855bc97e4618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-26 13:52:55.435589: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1180 MB memory:  -> device: 0, name: NVIDIA RTX A2000, pci bus id: 0000:b3:00.0, compute capability: 8.6\n",
      "Extracting dataset from .h5 file:   0%|                                                                                                                         | 0/1 [00:00<?, ?it/s]\n",
      "Creating dataset:   0%|                                                                                                                                        | 0/11 [00:00<?, ?it/s]\u001b[AWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1727369579.071090 2002206 service.cc:145] XLA service 0x7fce18004a00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1727369579.071162 2002206 service.cc:153]   StreamExecutor device (0): NVIDIA RTX A2000, Compute Capability 8.6\n",
      "2024-09-26 13:52:59.111865: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-09-26 13:52:59.248202: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "I0000 00:00:1727369581.584016 2002206 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "/home/rafaelfc/Data/DeepFlowImaging/FRCNN/examples/../../UNET/prediction/recreate_UNET_image.py:40: RuntimeWarning: invalid value encountered in divide\n",
      "  UNET_image /= UNET_image_nmax\n",
      "\n",
      "Creating dataset:   9%|███████████▋                                                                                                                    | 1/11 [00:07<01:16,  7.69s/it]\u001b[A\n",
      "Creating dataset:  18%|███████████████████████▎                                                                                                        | 2/11 [00:11<00:47,  5.25s/it]\u001b[A\n",
      "Creating dataset:  27%|██████████████████████████████████▉                                                                                             | 3/11 [00:14<00:34,  4.33s/it]\u001b[A\n",
      "Creating dataset:  36%|██████████████████████████████████████████████▌                                                                                 | 4/11 [00:17<00:27,  3.86s/it]\u001b[A\n",
      "Creating dataset:  45%|██████████████████████████████████████████████████████████▏                                                                     | 5/11 [00:20<00:21,  3.60s/it]\u001b[A\n",
      "Creating dataset:  55%|█████████████████████████████████████████████████████████████████████▊                                                          | 6/11 [00:23<00:17,  3.45s/it]\u001b[A\n",
      "Creating dataset:  64%|█████████████████████████████████████████████████████████████████████████████████▍                                              | 7/11 [00:27<00:13,  3.39s/it]\u001b[A\n",
      "Creating dataset:  73%|█████████████████████████████████████████████████████████████████████████████████████████████                                   | 8/11 [00:30<00:09,  3.32s/it]\u001b[A\n",
      "Creating dataset:  82%|████████████████████████████████████████████████████████████████████████████████████████████████████████▋                       | 9/11 [00:33<00:06,  3.24s/it]\u001b[A\n",
      "Creating dataset:  91%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍           | 10/11 [00:36<00:03,  3.24s/it]\u001b[A\n",
      "Creating dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:39<00:00,  3.61s/it]\u001b[A\n",
      "Extracting dataset from .h5 file: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:39<00:00, 39.77s/it]\n"
     ]
    }
   ],
   "source": [
    "create_dataset(h5_dataset_path, output_path, N_VALIDATION, N_VERIFICATION, UNET_model_options=UNET_model_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101d3438-b91a-4b2d-9efc-f526596a65f6",
   "metadata": {},
   "source": [
    "By running this function, you'll see that it created three new folders on the chosen path:\n",
    " - Training: Samples used during the F-RCNN training\n",
    " - Validation: Samples used for validation purpouses during training - Total of `N_VALIDATION` full images\n",
    " - Verification: Samples used to evaluate the F-RCNN accuracy after the training step. Unseen data during training  - Total of `N_VERIFICATION` full images\n",
    "\n",
    "\n",
    "\n",
    "## Option 1\n",
    "**If the `UNET_model_options` <span style=\"color:red\">is not</span> defined**, the F-RCNN object detection is based on the original image. \n",
    "In this case, folder cotains 3 folders:\n",
    " - `contours`: Text files which contains coordinates and parameters of the contours\n",
    " - `debug`: All the labelled images from the `.h5` file\n",
    " - `images`: All the raw images from the `.h5` file\n",
    "\n",
    "## Option 2\n",
    "**If the `UNET_model_options` <span style=\"color:red\">is </span> defined**, the F-RCNN object detection is based on the U-Net (segmented) representation of the image. \n",
    "In this case, folder cotains 4 folders:\n",
    " - `contours`: Text files which contains coordinates and parameters of the contours\n",
    " - `debug`: All the labelled images from the `.h5` file\n",
    " - `images`: All the raw images from the `.h5` file\n",
    " - `masks`: U-Net segmented images from images originally labeled in the `.h5` file\n",
    "\n",
    "In both options, the folder created in this step should be sent to the `FRCNN/dataset` folder for training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
